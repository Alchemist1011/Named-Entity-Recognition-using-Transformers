{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install the open source datasets library from HuggingFace**"
      ],
      "metadata": {
        "id": "WDt-QEsrY2lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5m-SBIhYnYF",
        "outputId": "6d458180-e25c-485e-d803-9ed7775199f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "--2024-09-17 16:13:23--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py.2’\n",
            "\n",
            "conlleval.py.2      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-17 16:13:23 (90.5 MB/s) - ‘conlleval.py.2’ saved [7502/7502]\n",
            "\n",
            "Requirement already satisfied: cudf-cu12 in /usr/local/lib/python3.10/dist-packages (24.8.3)\n",
            "Requirement already satisfied: ibis-framework in /usr/local/lib/python3.10/dist-packages (9.5.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (5.5.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2024.6.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.60.0)\n",
            "Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.3dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2.1.4)\n",
            "Requirement already satisfied: pyarrow<16.2.0a0,>=16.1.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (16.1.0)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (13.8.1)\n",
            "Requirement already satisfied: rmm-cu12==24.8.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.8.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (4.12.2)\n",
            "Requirement already satisfied: atpublic<6,>=2.3 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (4.1.0)\n",
            "Requirement already satisfied: parsy<3,>=2 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (2.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2022.7 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (2024.2)\n",
            "Requirement already satisfied: sqlglot<25.21,>=23.4 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (25.20.1)\n",
            "Requirement already satisfied: toolz<1,>=0.11 in /usr/local/lib/python3.10/dist-packages (from ibis-framework) (0.12.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12) (3.0.11)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12) (0.43.0)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.3dev0,>=2.0->cudf-cu12) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.8.2->ibis-framework) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install datasets\n",
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "!pip install --upgrade cudf-cu12 ibis-framework"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Module**"
      ],
      "metadata": {
        "id": "kzVqSsEVZPLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import ops\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "id": "ehGUTEdwZAHW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's start by defining a TransformerBlock layer**"
      ],
      "metadata": {
        "id": "NNJO1GLdZg6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)"
      ],
      "metadata": {
        "id": "LWc3LkehZbCH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "vOn3VOXUZorK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a Token and PositionEmbedding layer**"
      ],
      "metadata": {
        "id": "BEO20vmKZs-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = keras.layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n"
      ],
      "metadata": {
        "id": "dAmwSfgkZvQX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self, inputs):\n",
        "        maxlen = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings"
      ],
      "metadata": {
        "id": "UDsdiSx4Z4iK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the NER model class as a keras.Model subclass**"
      ],
      "metadata": {
        "id": "XA6r5Z-iZ-U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n"
      ],
      "metadata": {
        "id": "2XlOEfrxZ61t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "e6GkOX1laFiy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Load the CoNLL 2003 dataset from the datasets library**"
      ],
      "metadata": {
        "id": "ydeTbWMHaHLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "YGT9P5VHaQJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2905e2d-d661-49c9-fca3-fa70ad87bbab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            if len(tokens) > 0:\n",
        "                f.write(\n",
        "                    str(len(tokens))\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(tokens)\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(map(str, ner_tags))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "\n",
        "\n",
        "# Check if directory exists, create if not\n",
        "if not os.path.exists(\"data\"):\n",
        "    os.mkdir(\"data\")\n",
        "\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ],
      "metadata": {
        "id": "x1z1YMrGan-C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Make the NER label lookup table**"
      ],
      "metadata": {
        "id": "nOVmQpwSaqNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU069nB9atc6",
        "outputId": "b9227eaa-2d1f-4114-f7ff-8f920e2cb457"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "\n",
        "# We only take (vocab_size - 2) most commons words from the training data since\n",
        "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
        "# token and another one denoting a masking token\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "# The StringLook class will convert tokens to token IDs\n",
        "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha6nF08ua3Kt",
        "outputId": "e9227863-d882-4be5-bbb4-fb98b65ff999"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ],
      "metadata": {
        "id": "qAVXAVsPa7Jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474ec9dc-f08e-4215-bfa2-d6f0e185c2a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_data.take(1).as_numpy_iterator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9gtxKsXbB_R",
        "outputId": "0b97ba21-4a03-4a58-be0e-ecd8f3b30cb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "# We use `padded_batch` here because each record in the dataset has a\n",
        "# different length.\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
      ],
      "metadata": {
        "id": "s7enAvfDbGPJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False, reduction=None\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = ops.cast((y_true > 0), dtype=\"float32\")\n",
        "        loss = loss * mask\n",
        "        return ops.sum(loss) / ops.sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()"
      ],
      "metadata": {
        "id": "U3Zia52obIjZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile and fit the model**"
      ],
      "metadata": {
        "id": "F5ZORadGbLF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure eager execution is enabled\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Compile the NER model\n",
        "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
        "\n",
        "# Train the model\n",
        "history = ner_model.fit(train_dataset, epochs=10)\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_and_convert_to_ids(text):\n",
        "    \"\"\"Tokenize input text and convert to IDs.\"\"\"\n",
        "    tokens = text.lower().split()  # Convert to lowercase\n",
        "    return lowercase_and_convert_to_ids(tokens)\n",
        "\n",
        "# Define lowercase_and_convert_to_ids function (missing in original code)\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    # Assuming you have a vocabulary or tokenizer\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(tokens)\n",
        "    return tokenizer.texts_to_sequences([tokens])[0]\n",
        "\n",
        "# Sample inference using the trained model\n",
        "sample_input_text = \"eu rejects german call to boycott british lamb\"\n",
        "sample_input = tokenize_and_convert_to_ids(sample_input_text)\n",
        "sample_input = ops.reshape(sample_input, shape=[1, -1])\n",
        "\n",
        "# Predict using the trained model\n",
        "output = ner_model.predict(sample_input)\n",
        "prediction = np.argmax(output, axis=-1)[0]\n",
        "\n",
        "# Map predicted IDs to labels using the mapping dictionary\n",
        "predicted_labels = [mapping[i] for i in prediction]\n",
        "\n",
        "print(\"Input Text:\", sample_input_text)\n",
        "print(\"Predicted Labels:\", predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "f3HdAQUybMkX",
        "outputId": "ca56ea2d-1ef2-4509-ae03-ec32e8e9bd2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling NERModel.call().\n\n\u001b[1mModel NERModel does not have a `call()` method implemented.\u001b[0m\n\nArguments received by NERModel.call():\n  • args=('tf.Tensor(shape=(32, 47), dtype=int64)',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ecfa81ae9d3f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Define tokenization function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\"Model {self.__class__.__name__} does not have a `call()` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"method implemented.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling NERModel.call().\n\n\u001b[1mModel NERModel does not have a `call()` method implemented.\u001b[0m\n\nArguments received by NERModel.call():\n  • args=('tf.Tensor(shape=(32, 47), dtype=int64)',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics calculation**"
      ],
      "metadata": {
        "id": "K8gx_4rKctRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(dataset):\n",
        "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
        "\n",
        "    for x, y in dataset:\n",
        "        output = ner_model.predict(x, verbose=0)\n",
        "        predictions = ops.argmax(output, axis=-1)\n",
        "        predictions = ops.reshape(predictions, [-1])\n",
        "\n",
        "        true_tag_ids = ops.reshape(y, [-1])\n",
        "\n",
        "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
        "        true_tag_ids = true_tag_ids[mask]\n",
        "        predicted_tag_ids = predictions[mask]\n",
        "\n",
        "        all_true_tag_ids.append(true_tag_ids)\n",
        "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
        "\n",
        "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
        "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
        "\n",
        "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
        "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
        "\n",
        "    evaluate(real_tags, predicted_tags)\n",
        "\n",
        "\n",
        "calculate_metrics(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "XA8HK7Xpcwrs",
        "outputId": "6bb4a9c7-158f-4559-dcc3-9c6c8e0c8126"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling NERModel.call().\n\n\u001b[1mModel NERModel does not have a `call()` method implemented.\u001b[0m\n\nArguments received by NERModel.call():\n  • args=('tf.Tensor(shape=(32, 41), dtype=int64)',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-fdcc1444ac29>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-fdcc1444ac29>\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         raise NotImplementedError(\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;34mf\"Model {self.__class__.__name__} does not have a `call()` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"method implemented.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling NERModel.call().\n\n\u001b[1mModel NERModel does not have a `call()` method implemented.\u001b[0m\n\nArguments received by NERModel.call():\n  • args=('tf.Tensor(shape=(32, 41), dtype=int64)',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    }
  ]
}